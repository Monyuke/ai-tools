"""
title: Web Search and URL Tool (No Dependencies)
author: monyuke
author_url:
funding_url:
version: 0.2.1
"""

"""
ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå½¢å¼ãŒã€Œãƒã‚¤ãƒ†ã‚£ãƒ–ã€ã˜ã‚ƒãªã„ã¨ä½¿ãˆãªã„ã®ã§æ³¨æ„ï¼
"""

import urllib.request
import urllib.parse
import urllib.error
import json
import re
import html
from html.parser import HTMLParser
from ddgs import DDGS
from typing import Optional
import zlib
from langchain.tools import tool


def extract_text_from_pdf(pdf_data: bytes) -> str:
    """
    PDFãƒã‚¤ãƒŠãƒªã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ä½¿ç”¨ï¼‰

    Parameters
    ----------
    pdf_data: bytes
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿

    Returns
    -------
    str
        æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        text_parts = []
        pdf_str = pdf_data.decode("latin-1", errors="ignore")

        # PDFã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: stream...endstream ã®é–“ã®ãƒ‡ãƒ¼ã‚¿
        stream_pattern = re.compile(rb"stream\r?\n(.*?)\r?\nendstream", re.DOTALL)

        for match in stream_pattern.finditer(pdf_data):
            stream_data = match.group(1)

            # FlateDecodeï¼ˆzlibåœ§ç¸®ï¼‰ã®è§£å‡ã‚’è©¦ã¿ã‚‹
            try:
                decompressed = zlib.decompress(stream_data)
                stream_text = decompressed.decode("latin-1", errors="ignore")
            except:
                stream_text = stream_data.decode("latin-1", errors="ignore")

            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
            # PDFã®ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒšãƒ¬ãƒ¼ã‚¿: Tj, TJ, ' ãªã©
            text_operators = re.findall(
                r"\(((?:[^()\\]|\\.)*)\)\s*T[jJ\']", stream_text
            )

            for text in text_operators:
                # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†
                decoded_text = (
                    text.replace("\\(", "(").replace("\\)", ")").replace("\\\\", "\\")
                )
                if decoded_text.strip():
                    text_parts.append(decoded_text)

            # é…åˆ—å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆ: [(text1)(text2)] TJ
            array_pattern = re.findall(
                r"\[((?:\([^)]*\)\s*-?\d*\s*)+)\]\s*TJ", stream_text
            )
            for array_text in array_pattern:
                texts = re.findall(r"\(([^)]*)\)", array_text)
                for text in texts:
                    decoded_text = (
                        text.replace("\\(", "(")
                        .replace("\\)", ")")
                        .replace("\\\\", "\\")
                    )
                    if decoded_text.strip():
                        text_parts.append(decoded_text)

        # æŠ½å‡ºã§ããªã‹ã£ãŸå ´åˆã¯å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
        if not text_parts:
            # obj...endobj ã®é–“ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚‰ã—ãã‚‚ã®ã‚’æŠ½å‡º
            simple_text = re.findall(r"\(([^)]{3,})\)", pdf_str)
            text_parts = [t for t in simple_text if len(t.strip()) > 2]

        return " ".join(text_parts)

    except Exception as e:
        print(f"[extract_text_from_pdf] ã‚¨ãƒ©ãƒ¼: {e}")
        return ""


def fetch_text_sync(
    url: str,
    *,
    timeout: int = 60,  # 60 ç§’
) -> str:
    """
    æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§URLã‚’èª­ã¿è¾¼ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚
    PDFã®å ´åˆã¯æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è¡Œã†ã€‚

    Parameters
    ----------
    url: str
        å–å¾—å¯¾è±¡ã® URL
    timeout: int, default 60
        ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

    Returns
    -------
    str
        ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‚å–å¾—å¤±æ•—æ™‚ã¯ç©ºæ–‡å­—åˆ—ã€‚
    """
    try:
        # PDFã‹ã©ã†ã‹ã‚’URLã®æ‹¡å¼µå­ã§åˆ¤å®š
        is_pdf = url.lower().endswith(".pdf")

        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/91.0.4472.124 Safari/537.36"
            )
        }

        # URLã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        if not urllib.parse.urlparse(url).scheme:
            url = "https://" + url

        url = encode_url(url)
        req = urllib.request.Request(url, headers=headers)

        with urllib.request.urlopen(req, timeout=timeout) as response:
            content = response.read()

            if is_pdf:
                # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                return extract_text_from_pdf(content)
            else:
                # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆbodyå†…ã®ã¿ï¼‰
                html_content = decode_content(content, response.headers)
                extractor = TextExtractor(body_only=True)
                extractor.feed(html_content)
                text = " ".join(extractor.text)
                return text

    except urllib.error.HTTPError as e:
        print(f"[fetch_text_sync] HTTP Error {e.code}: {e.reason}")
        return ""
    except urllib.error.URLError as e:
        print(f"[fetch_text_sync] URL Error: {str(e.reason)}")
        return ""
    except Exception as e:
        print(f"[fetch_text_sync] ã‚¨ãƒ©ãƒ¼: {e}")
        return ""


# ----------------------------------------------------------------------
# URL ã‚’ ASCII æ–‡å­—åˆ—ã¸å¤‰æ›ï¼ˆPunycode + percentâ€‘encodeï¼‰
# ----------------------------------------------------------------------
def encode_url(url: str) -> str:
    """
    URL ã‚’ ASCII æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯ Punycodeã€path ã¯
    percentâ€‘encode ã™ã‚‹ã€‚ã‚¹ã‚­ãƒ¼ãƒãŒç„¡ã‘ã‚Œã° https:// ã‚’ä»˜ä¸ã€‚
    """
    # ã‚¹ã‚­ãƒ¼ãƒãŒç„¡ã„å ´åˆã¯ https:// ã‚’ä»˜ä¸
    if not urllib.parse.urlparse(url).scheme:
        url = "https://" + url

    parsed = urllib.parse.urlparse(url)

    # netlocï¼ˆãƒ›ã‚¹ãƒˆéƒ¨åˆ†ï¼‰ã‚’ Punycode ã¸å¤‰æ›
    try:
        netloc = parsed.netloc.encode("idna").decode("ascii")
    except Exception:
        netloc = parsed.netloc

    # path ã‚’ percentâ€‘encode (unsafe ã¯ç©ºæ–‡å­—ã«ã—ã¦å…¨ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰)
    path = urllib.parse.quote(parsed.path, safe="/%-._~!$&'()*+,;=:@")

    # query ã¯ RFC3986 ã® safe characters ã‚’æ®‹ã—ã¤ã¤ encode
    query = urllib.parse.quote_plus(parsed.query, safe="=&")

    # å†æ§‹ç¯‰ã—ã¦ ASCII æ–‡å­—åˆ—ã¸
    encoded = urllib.parse.urlunparse(
        (
            parsed.scheme,
            netloc,
            path,
            parsed.params,
            query,
            parsed.fragment,
        )
    )
    return encoded


# ----------------------------------------------------------------------
# æ–‡å­—ã‚³ãƒ¼ãƒ‰åˆ¤å®šãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
# ----------------------------------------------------------------------
def decode_content(content: bytes, headers) -> str:
    """ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ãªæ–‡å­—ã‚³ãƒ¼ãƒ‰ã§ãƒ‡ã‚³ãƒ¼ãƒ‰"""
    # 1. ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ charset ã‚’å–å¾—
    charset = None
    if hasattr(headers, "get_content_charset"):
        charset = headers.get_content_charset()
    if not charset:
        ct = headers.get("Content-Type", "")
        match = re.search(r"charset=([^\s;]+)", ct, re.IGNORECASE)
        if match:
            charset = match.group(1)

    # 2. å–å¾—ã§ããŸ charset ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è©¦è¡Œ
    if charset:
        try:
            return content.decode(charset, errors="replace")
        except Exception:
            pass

    # 3. ä¸€èˆ¬çš„ãªæ—¥æœ¬èªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§é †ç•ªã«è©¦è¡Œ
    for enc in ["utf-8", "utf-8-sig", "shift_jis", "euc-jp", "iso-2022-jp"]:
        try:
            return content.decode(enc, errors="replace")
        except Exception:
            continue

    # 4. æœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    return content.decode("utf-8", errors="replace")


# ----------------------------------------------------------------------
# HTML ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
# ----------------------------------------------------------------------
class TextExtractor(HTMLParser):
    def __init__(self, body_only=False):
        super().__init__()
        self.text = []
        self.links = []
        self.current_link = None
        self.body_only = body_only
        self.in_body = False
        self.in_script = False
        self.in_style = False

    def handle_starttag(self, tag, attrs):
        if tag == "body":
            self.in_body = True
        elif tag == "script":
            self.in_script = True
        elif tag == "style":
            self.in_style = True
        elif tag == "a" and (not self.body_only or self.in_body):
            for attr, value in attrs:
                if attr == "href":
                    self.current_link = value
                    break

    def handle_endtag(self, tag):
        if tag == "body":
            self.in_body = False
        elif tag == "script":
            self.in_script = False
        elif tag == "style":
            self.in_style = False
        elif tag == "a":
            self.current_link = None

    def handle_data(self, data):
        # script, styleã‚¿ã‚°å†…ã¯ç„¡è¦–
        if self.in_script or self.in_style:
            return

        # body_onlyãŒTrueã®å ´åˆã€bodyå†…ã®ã¿æŠ½å‡º
        if self.body_only and not self.in_body:
            return

        clean_data = data.strip()
        if clean_data:
            self.text.append(clean_data)
            if self.current_link and clean_data:
                self.links.append({"text": clean_data, "url": self.current_link})


# ----------------------------------------------------------------------
# ãƒ„ãƒ¼ãƒ«ã‚¯ãƒ©ã‚¹ï¼ˆå†…éƒ¨ã«ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã¯ç½®ã‹ãªã„ï¼‰
# ----------------------------------------------------------------------
class Tools:
    def __init__(self):
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/91.0.4472.124 Safari/537.36"
            )
        }

    def fetch_webpage(
        self,
        url: str,
        part: int = 1,
    ) -> str:
        """
        Fetch and return the text content of a webpage using Playwright.
        :param url: The URL to fetch.
        :param part: The block number to retrieve (starting from 1).
        :return: Text content of the webpage
        """
        part_size: int = 2000
        try:
            # Playwrightã§ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            text = fetch_text_sync(url)

            if not text:
                return f"Error: Could not fetch content from {url}"

            # ç©ºç™½ã‚’æ­£è¦åŒ–
            text = re.sub(r"\s+", " ", text).strip()

            # part ç•ªå·ã¯ 1 ã‹ã‚‰å§‹ã¾ã‚‹æƒ³å®š
            if part < 1:
                return f"Error: part must be >= 1"

            # å…¨ä½“ã®ãƒ‘ãƒ¼ãƒˆæ•°ã‚’è¨ˆç®—
            total_parts = max(1, (len(text) + part_size - 1) // part_size)

            start = (part - 1) * part_size
            end = start + part_size

            # æ–‡å­—åˆ—é•·ã«åˆã‚ã›ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è£œæ­£
            if start >= len(text):
                # æŒ‡å®šã—ãŸ part ãŒå­˜åœ¨ã—ãªã„
                return (
                    f"Content from {url} (part {part} of {total_parts}):\n\n"
                    f"[No more content; part {part} is out of range]"
                )
            if end > len(text):
                end = len(text)

            sliced_text = text[start:end]
            return (
                f"Content from {url} (part {part} of {total_parts}):\n\n"
                f"{sliced_text}"
            )
        except Exception as e:
            return f"Error fetching webpage: {str(e)}"

    # ------------------------------------------------------------------
    #  ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯æŠ½å‡º
    # ------------------------------------------------------------------
    def extract_links(self, url: str) -> str:
        """
        Extract all links from a webpage using built-in libraries.
        :param url: URL of the webpage to extract links from
        :return: List of links found on the page
        """
        try:
            encoded_url = encode_url(url)

            req = urllib.request.Request(encoded_url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read()
                html_content = decode_content(content, response.headers)

                extractor = TextExtractor()
                extractor.feed(html_content)

                unique_links = []
                seen_urls = set()
                for link in extractor.links:
                    href = link["url"]
                    text = link["text"]

                    if href.startswith("/"):
                        href = urllib.parse.urljoin(encoded_url, href)
                    elif not href.startswith(("http://", "https://")):
                        continue

                    if href not in seen_urls and text:
                        unique_links.append({"text": text, "url": href})
                        seen_urls.add(href)

                if not unique_links:
                    return f"No links found on {url}"

                result = f"Links found on {url}:\n\n"
                for i, link in enumerate(unique_links[:20], 1):
                    result += f"{i}. {link['text']}\n   {link['url']}\n\n"

                if len(unique_links) > 20:
                    result += f"... and {len(unique_links) - 20} more links"

                return result
        except urllib.error.HTTPError as e:
            return f"HTTP Error {e.code}: {e.reason}"
        except urllib.error.URLError as e:
            return f"URL Error: {str(e.reason)}"
        except Exception as e:
            return f"Error extracting links: {str(e)}"

    # ------------------------------------------------------------------
    #  URL ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
    # ------------------------------------------------------------------
    def check_url_status(self, url: str) -> str:
        """
        Check if a URL is accessible and return status information.
        :param url: URL to check
        :return: Status information about the URL
        """
        try:
            encoded_url = encode_url(url)

            req = urllib.request.Request(encoded_url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                status_code = response.getcode()
                content_length = len(response.read())
                content_type = response.headers.get("Content-Type", "Unknown")
                server = response.headers.get("Server", "Unknown")

                status_info = f"URL Status Check for {url}:\n"
                status_info += f"ğŸŒ Status Code: {status_code}\n"
                status_info += f"ğŸ“ Content Length: {content_length} bytes\n"
                status_info += f"ğŸ“„ Content Type: {content_type}\n"
                status_info += f"ğŸ”§ Server: {server}\n"

                if status_code == 200:
                    status_info += "âœ… URL is accessible"
                else:
                    status_info += f"âš ï¸ URL returned status code {status_code}"
                return status_info
        except urllib.error.HTTPError as e:
            return f"âŒ HTTP Error {e.code}: {e.reason}"
        except urllib.error.URLError as e:
            return f"âŒ URL Error: {str(e.reason)}"
        except Exception as e:
            return f"âŒ Error checking URL: {str(e)}"

    def web_search(self, query: str) -> str:
        """
        Perform a search on DuckDuckGo and return the results in HTML.
        :param query: Search query.
        :return: List of dictionaries with search results.
        """
        return DDGS().text(query, max_results=10, backend="duckduckgo")

    # ------------------------------------------------------------------
    #  ãƒšãƒ¼ã‚¸å†…æ¤œç´¢
    # ------------------------------------------------------------------
    def find_in_page(self, pattern: str, url: str) -> str:
        """
        Search for a pattern in a webpage and return matching lines with context.
        :param pattern: Search pattern (regular expression supported)
        :param url: URL of the webpage to search
        :return: Matching lines with surrounding context
        """
        try:
            # Playwrightã§ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            text = fetch_text_sync(url)

            if not text:
                return f"Error: Could not fetch content from {url}"

            # ç©ºç™½ã‚’æ­£è¦åŒ–
            text = re.sub(r"\s+", " ", text).strip()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ï¼ˆæ­£è¦è¡¨ç¾å¯¾å¿œï¼‰
            try:
                matches = list(re.finditer(pattern, text, re.IGNORECASE))
            except re.error:
                return f"Error: Invalid regular expression pattern '{pattern}'"

            if not matches:
                return f"Pattern '{pattern}' not found in {url}"

            result = f"Found {len(matches)} match(es) for '{pattern}' in {url}:\n\n"

            # æœ€å¤§10ä»¶ã®ãƒãƒƒãƒã‚’è¡¨ç¤º
            for i, match in enumerate(matches[:10], 1):
                start = max(0, match.start() - 100)
                end = min(len(text), match.end() + 100)
                context = text[start:end]

                # ãƒãƒƒãƒéƒ¨åˆ†ã‚’å¼·èª¿
                highlighted = (
                    context[: match.start() - start]
                    + f">>>{match.group()}<<<"
                    + context[match.end() - start :]
                )

                result += f"{i}. ...{highlighted}...\n\n"

            if len(matches) > 10:
                result += f"... and {len(matches) - 10} more match(es)"

            return result

        except Exception as e:
            return f"Error searching in page: {str(e)}"


@tool
def web_search(query: str) -> str:
    """
    Perform a search on DuckDuckGo and return the results in HTML.
    :param query: Search query.
    :return: List of dictionaries with search results.
    """
    return DDGS().text(query, max_results=10, backend="duckduckgo")


@tool
def find_in_page(pattern: str, url: str) -> str:
    """
    Search for a pattern in a webpage and return matching lines with context.
    :param pattern: Search pattern (regular expression supported)
    :param url: URL of the webpage to search
    :return: Matching lines with surrounding context
    """
    return Tools().find_in_page(pattern, url)


@tool
def fetch_webpage(
    url: str,
    part: int = 1,
) -> str:
    """
    Fetch and return the text content of a webpage using Playwright.
    :param url: The URL to fetch.
    :param part: The block number to retrieve (starting from 1).
    :return: Text content of the webpage
    """
    return Tools().fetch_webpage(url, part)
